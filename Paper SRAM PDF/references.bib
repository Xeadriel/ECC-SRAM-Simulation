@article{10.1145/3623402,
author = {Rokh, Babak and Azarpeyvand, Ali and Khanteymoori, Alireza},
title = {A Comprehensive Survey on Model Quantization for Deep Neural Networks in Image Classification},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3623402},
doi = {10.1145/3623402},
abstract = {Recent advancements in machine learning achieved by Deep Neural Networks (DNNs) have been significant. While demonstrating high accuracy, DNNs are associated with a huge number of parameters and computations, which leads to high memory usage and energy consumption. As a result, deploying DNNs on devices with constrained hardware resources poses significant challenges. To overcome this, various compression techniques have been widely employed to optimize DNN accelerators. A promising approach is quantization, in which the full-precision values are stored in low bit-width precision. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods. Since quantization has been extensively utilized in previous works, there is a need for an integrated report that provides an understanding, analysis, and comparison of different quantization approaches. Consequently, we present a comprehensive survey of quantization concepts and methods, with a focus on image classification. We describe clustering-based quantization methods and explore the use of a scale factor parameter for approximating full-precision values. Moreover, we thoroughly review the training of a quantized DNN, including the use of a straight-through estimator and quantization regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. Furthermore, we highlight the evaluation metrics for quantization methods and important benchmarks in the image classification task. We also present the accuracy of the state-of-the-art methods on CIFAR-10 and ImageNet. This article attempts to make the readers familiar with the basic and advanced concepts of quantization, introduce important works in DNN quantization, and highlight challenges for future research in this field.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {97},
numpages = {50},
keywords = {Quantization, model compression, deep neural network acceleration, image classification, discrete neural network optimization}
}

@INPROCEEDINGS{jiwuSRAM,
  author={Wu, Ji and Namba, Kazuteru},
  booktitle={2023 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)}, 
  title={SRAM-based efficiency memory model for quantized convolutional neural networks}, 
  year={2023},
  volume={},
  number={},
  pages={499-500},
  keywords={Power demand;Bit error rate;Neural networks;Memory management;Reliability engineering;Internet of Things;Convolutional neural networks;CNN;SRAM;Quantization},
  doi={10.1109/ICCE-Taiwan58799.2023.10226859}}


@misc{jacob2017quantizationtrainingneuralnetworks,
      title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}, 
      author={Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
      year={2017},
      eprint={1712.05877},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.05877}, 
}

@INPROCEEDINGS{yangSRAM,
  author={Yang, Lita and Murmann, Boris},
  booktitle={2017 18th International Symposium on Quality Electronic Design (ISQED)}, 
  title={SRAM voltage scaling for energy-efficient convolutional neural networks}, 
  year={2017},
  volume={},
  number={},
  pages={7-12},
  keywords={Random access memory;Training;Bit error rate;Micromechanical devices;Hardware;Deep learning;convolutional neural networks;error resiliency;approximate SRAM;energy-quality tradeoff},
  doi={10.1109/ISQED.2017.7918284}
  }
  
  @ARTICLE{726791,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}

